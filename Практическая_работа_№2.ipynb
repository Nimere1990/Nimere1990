{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOY1hNTI1f/bGECQ8d48o9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nimere1990/Nimere1990/blob/main/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%962.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Практическая работа №2*\n",
        "\n",
        "Нейро-сотрудник на базе SAIGA 7B\n",
        "\n",
        "Нейро - сотрудник - юрист консультант в области строительства.\n",
        "\n",
        "Основные документы для работы нейро-сотрудника:\n",
        "\n",
        "Конституция РФ\n",
        "\n",
        "https://constitutionrf.ru/constitutionrf.pdf\n",
        "\n",
        "Жилищный кодекс\n",
        "\n",
        "https://mtseti.ru/images/docs/norm/Zil-kodeks-s-izm-01-03-23.pdf\n",
        "\n",
        "Земельный кодекс\n",
        "\n",
        "https://rpn.gov.ru/upload/iblock/ffd/h36g7vjmm0n7m4gqfcib0si1umee7fp6/Zemelnyy-kodeks-Rossiyskoy-Federatsii-ot-25.10.2001-N-136_F.pdf\n",
        "\n",
        "Градостроительный кодекс\n",
        "\n",
        "https://rpn.gov.ru/upload/iblock/743/1lxwwqd0nxbzssd3r4zknmz68fj4mwif/Gradostroitelnyy-kodeks-Rossiyskoy-Federatsii-ot-29.12.2004-_-190_FZ.pdf\n",
        "\n",
        "Гражданский кодекс\n",
        "\n",
        "https://www.garant.ru/files/9/2/343529/garant_grajdansky_kodeks_rf.pdf\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BM5ZSk8J4KjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IUz2Gqd1lLI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install llama_index pyvis Ipython langchain pypdf langchain_community\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-embeddings-langchain\n",
        "!pip install langchain-huggingface\n",
        "!pip install sentencepiece accelerate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install llama-index-readers-smart-pdf-loader\n",
        "!pip install llmsherpa\n",
        "!pip install torch\n",
        "!pip install transformers datasets accelerate evaluate bitsandbytes trl peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import KnowledgeGraphIndex\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.graph_stores import SimpleGraphStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader # для загрузки файла и его векторизации\n",
        "from llama_index.core.postprocessor import LLMRerank # модуль реранжирования на базе LLM\n",
        "from llama_index.readers.smart_pdf_loader import SmartPDFLoader\n",
        "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from pyvis.network import Network\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n"
      ],
      "metadata": {
        "id": "3JVCufk1pMvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN=\"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\"\n",
        "# Вставьте ваш токен (здесь указан временный токен)\n",
        "login(HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "PpolbhHhpDWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_to_prompt(messages):\n",
        "    prompt = \"\"\n",
        "    for message in messages:\n",
        "        if message.role == 'system':\n",
        "            prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'user':\n",
        "            prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'bot':\n",
        "            prompt += f\"<s>bot\\n\"\n",
        "\n",
        "    # ensure we start with a system prompt, insert blank if needed\n",
        "    if not prompt.startswith(\"<s>system\\n\"):\n",
        "        prompt = \"<s>system\\n</s>\\n\" + prompt\n",
        "\n",
        "    # add final assistant prompt\n",
        "    prompt = prompt + \"<s>bot\\n\"\n",
        "    return prompt\n",
        "\n",
        "def completion_to_prompt(completion):\n",
        "    return f\"<s>system\\n</s>\\n<s>user\\n{completion}</s>\\n<s>bot\\n\""
      ],
      "metadata": {
        "id": "9T4DMaXd5Ln_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Задаем имя модели\n",
        "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
        "\n",
        "# Создание конфига, соответствующего методу PEFT (в нашем случае LoRA)\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Загружаем базовую модель, ее имя берем из конфига для LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,          # идентификатор модели\n",
        "    quantization_config=quantization_config, # параметры квантования\n",
        "    torch_dtype=torch.float16,               # тип данных\n",
        "    device_map=\"auto\"                        # автоматический выбор типа устройства\n",
        ")\n",
        "\n",
        "# Загружаем LoRA модель\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Переводим модель в режим инференса\n",
        "# Можно не переводить, но явное всегда лучше неявного\n",
        "model.eval()\n",
        "\n",
        "# Загружаем токенизатор\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)"
      ],
      "metadata": {
        "id": "7QeoyP1E5VsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "print(generation_config)"
      ],
      "metadata": {
        "id": "03cPde8F5XbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    model=model,             # модель\n",
        "    model_name=MODEL_NAME,   # идентификатор модели\n",
        "    tokenizer=tokenizer,     # токенизатор\n",
        "    max_new_tokens=generation_config.max_new_tokens, # параметр необходимо использовать здесь, и не использовать в generate_kwargs, иначе ошибка двойного использования\n",
        "    model_kwargs={\"quantization_config\": quantization_config}, # параметры квантования\n",
        "    generate_kwargs = {   # параметры для инференса\n",
        "      \"bos_token_id\": generation_config.bos_token_id, # токен начала последовательности\n",
        "      \"eos_token_id\": generation_config.eos_token_id, # токен окончания последовательности\n",
        "      \"pad_token_id\": generation_config.pad_token_id, # токен пакетной обработки (указывает, что последовательность ещё не завершена)\n",
        "      \"no_repeat_ngram_size\": generation_config.no_repeat_ngram_size,\n",
        "      \"repetition_penalty\": generation_config.repetition_penalty,\n",
        "      \"temperature\": generation_config.temperature,\n",
        "      \"do_sample\": True,\n",
        "      \"top_k\": 50,\n",
        "      \"top_p\": 0.95\n",
        "    },\n",
        "    messages_to_prompt=messages_to_prompt,     # функция для преобразования сообщений к внутреннему формату\n",
        "    completion_to_prompt=completion_to_prompt, # функции для генерации текста\n",
        "    device_map=\"auto\",                         # автоматически определять устройство\n",
        ")"
      ],
      "metadata": {
        "id": "pIA89TvZ5Z_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/'\n",
        "!wget https://constitutionrf.ru/constitutionrf.pdf -O 'data/cons.pdf'\n",
        "!wget https://mtseti.ru/images/docs/norm/Zil-kodeks-s-izm-01-03-23.pdf -O 'data/zil.pdf'\n",
        "!wget https://rpn.gov.ru/upload/iblock/ffd/h36g7vjmm0n7m4gqfcib0si1umee7fp6/Zemelnyy-kodeks-Rossiyskoy-Federatsii-ot-25.10.2001-N-136_F.pdf -O 'data/zem.pdf'\n",
        "!wget https://rpn.gov.ru/upload/iblock/743/1lxwwqd0nxbzssd3r4zknmz68fj4mwif/Gradostroitelnyy-kodeks-Rossiyskoy-Federatsii-ot-29.12.2004-_-190_FZ.pdf -O 'data/grad.pdf'\n",
        "!wget https://www.garant.ru/files/9/2/343529/garant_grajdansky_kodeks_rf.pdf -O 'data/grazd.pdf'\n"
      ],
      "metadata": {
        "id": "0efgnm8IuZqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем все документы из папки\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()"
      ],
      "metadata": {
        "id": "7kKPRJOpjnuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface  import HuggingFaceEmbeddings\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        ")"
      ],
      "metadata": {
        "id": "pgYFQhzn5vuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка ServiceContext (глобальная настройка параметров LLM)\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512"
      ],
      "metadata": {
        "id": "7ZrujWqO5xAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем простое графовое хранилище\n",
        "graph_store = SimpleGraphStore()\n",
        "\n",
        "# Устанавливаем информацию о хранилище в StorageContext\n",
        "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
      ],
      "metadata": {
        "id": "Q6GwWuI351mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexKG = KnowledgeGraphIndex.from_documents( documents=documents,               # данные для построения графов\n",
        "                                           max_triplets_per_chunk=3,        # сколько обработывать триплетов связей для каждого блока данных\n",
        "                                           show_progress=True,              # показывать процесс выполнения\n",
        "                                           include_embeddings=True,         # включение векторных вложений в индекс для расширенной аналитики\n",
        "                                           storage_context=storage_context) # куда сохранять результаты"
      ],
      "metadata": {
        "id": "Sb9xn8do53j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvis.network import Network\n",
        "from IPython.display import display\n",
        "import IPython\n",
        "\n",
        "g = indexKG.get_networkx_graph(500)\n",
        "net = Network(notebook=True,cdn_resources=\"in_line\", directed=True)\n",
        "net.from_nx(g)\n",
        "net.show(\"graph.html\")\n",
        "net.save_graph(\"Knowledge_graph.html\")\n",
        "\n",
        "IPython.display.HTML(filename=\"/content/Knowledge_graph.html\")"
      ],
      "metadata": {
        "id": "W1m6egOO547H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Какие требования необходимо соблюдать при строительстве нового жилого дома в России?\"\n",
        "query_engine = indexKG.as_query_engine(include_text=True, verbose=True)\n",
        "#\n",
        "message_template =f\"\"\"<s>system\n",
        "Отвечай в соответствии с Источником. Проверь, есть ли в Источнике упоминания о ключевых словах Вопроса.\n",
        "Если нет, то просто скажи: 'я не знаю'. Не придумывай! </s>\n",
        "<s>user\n",
        "Вопрос: {query}\n",
        "Источник:\n",
        "</s>\n",
        "\"\"\"\n",
        "#\n",
        "response = query_engine.query(message_template)\n",
        "#\n",
        "print()\n",
        "print('Ответ:')\n",
        "print(response.response)"
      ],
      "metadata": {
        "id": "S1FvI2Pf56aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storage_context.persist() # сохраняем хранилище в файловую систему"
      ],
      "metadata": {
        "id": "g08oNaWx59ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}